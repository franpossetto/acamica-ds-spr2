{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ3nBaY56JOx"
   },
   "source": [
    "# Preprocesamiento de Datos\n",
    "\n",
    "El preprocesamiento de datos - a veces también conocido como transformación de datos o, incluso, ingeniería de atributos - es probablemente una de las parte más importante del trabajo del Data Scientist, tal vez la más importante. Como habrás visto en la bitácora, son muchos los pasos involucrados en este proceso, y eso que no los hemos mencionados a todos, pero sí los más importantes. En este notebook, trabajaremos sobre cuatro grandes áreas:\n",
    "\n",
    "1. Valores Faltantes\n",
    "2. Valores Atípicos\n",
    "3. Escalado de Datos\n",
    "4. Encoders\n",
    "\n",
    "Como este notebook corresponde a dos bitácoras y encuentros (19 y 20), no te preocupes si todavía no sabes de qué se tratan los puntos 3 y 4.\n",
    "\n",
    "Antes de arrancar, una pregunta usual que suele surgir es si usar Pandas o Scikit-Learn, ya que algunos de estos pasos pueden hacerse indistintamente con cualquiera de las dos librerías. Como siempre, la respuesta depende de lo que quieras hacer - y también con cuál herramienta te sientas más cómodo/a. Pero una respuesta rápida es que si estas explorando datos, mejor usar Pandas. Si estás en un flujo de Machine Learning, Scikit-Learn. Probablemente, termines usando las dos en un mismo notebook.\n",
    "\n",
    "#### Algunas referencias\n",
    "\n",
    "**Muy útil:** [documentación de Scikit-Learn sobre preprocesamiento de datos](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "**Valores Faltantes**\n",
    "\n",
    "* En Pandas, explora las funcionalidades `isna()`, `dropna()` y `fillna()`.\n",
    "* En Scikit-Learn, explora la clase `SimpleImputer`.\n",
    "\n",
    "**Valores Atípicos**\n",
    "\n",
    "* Para aplicar los criterios que mencionamos, bastará usar las técnicas de visualización que ya conoces, calcular percentiles, promedios y desviaciones estándar con NumPy o Pandas, y usar máscaras.\n",
    "* En Scikit-Learn existen métodos para trabajar con valores atípicos. Si bien no los utilizaremos, no está demás si quieres mirarlos [aquí](https://scikit-learn.org/stable/modules/outlier_detection.html).\n",
    "\n",
    "**Escalado de Datos**\n",
    "* Se puede hacer a mano con las herramientas de NumPy o Pandas\n",
    "* En Scikit-Learn existe una variedad de clases que pueden ayudarte en este proceso. Puedes mirar en el enlace a la documentación de Scikit-Learn que dejamos más arriba.\n",
    "\n",
    "**Encoders**\n",
    "* En Pandas, explora las funcionalidades `get_dummies()` y `map()`\n",
    "* En Scikit-Learn, explora la clase `LabelEncoder` y `OneHotEncoder`.\n",
    "\n",
    "\n",
    "## 1. Valores Faltantes\n",
    "\n",
    "En este ejercicio te proveemos de un dataset de seis columnas y 1500 instancias. Salvo una columna, `V1`, todas tienen valores faltantes. De las columnas con valores faltantes:\n",
    "* Dos tienen valores faltantes MCAR (el valor faltante es completamente al azar y no depende de otras variables)\n",
    "* Una tiene valores faltantes MAR (la probabilidad de valor faltante depende de otra variable).\n",
    "* Una tiene valores faltantes MNAR (la probabilidad de valor faltante depende de esa misma variable).\n",
    "\n",
    "### 1.1 MCAR, MAR o MNAR - **Opcional**\n",
    "\n",
    "El objetivo de esta sección es que analisis el origen de estos valores faltantes. Para ello:\n",
    "\n",
    "1. Abre los datos y explóralos. Con ese fin, te recomendamos hacer un `pairplot` (con y sin `hue`) de Seaborn y contar los valores faltantes por columna. ¿Qué tipo de datos son?¿Cuáles son sus distribuciones?¿Te parece que hay alguna columna correlacionada con otra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrcTcKyr6JO2"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdKsJ61X6JPL"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZIv74m66JPe"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRIggglI6JPs"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBFFC1TF6JP4"
   },
   "source": [
    "2. Buscaremos decidir el mecanismo que dio origen a los valores faltante de cada columna. Para ello, ten en cuenta que:\n",
    "* Para simplificar el análisis, si hay valores MAR, la dependencia sólo puede ser con la columna categórica sin valores faltantes, `V1`. Entonces, basta que solamente pruebes dependencia con esa variable. Te puede ser **muy útil** [esta pregunta de Stack Overflow](https://stackoverflow.com/questions/43321455/pandas-count-null-values-in-a-groupby-function/43322220).\n",
    "* MNAR puede es difícil de estudiar si no tenemos conocimiento del tema o si no hay una variable que correlacione con la variable que queremos estudiar para usar de *proxy*. ¿Qué queremos decir con esto? Imaginate que tienes una planilla con sueldos mensuales de personas y vas a borrar valores con mayor probabilidad si son más altos. Pero no te diste cuenta que en otra planilla figuran el estimativo del sueldo anual, lo cual correlaciona fuertemente con el sueldo mensual. Entonces, si quieres ver si la probabilidad de borrar un valor estaba relacionada con ese valor, lo que puedes hacer es crear intervalos para la variables sin valores faltantes - es decir, crear rangos de sueldos anuales - y contar cuántas instancias que caen dentro de ese intervalo tienen valores faltantes en la variable borrada (sueldo mensual). Al principio es muy confuso, pero piénsalo un rato y vas a ver que tiene sentido.\n",
    "* MCAR es un mecanismo que puede ser difícil de estudiar. Tal vez es más probable que llegues a él *por descarte* de los otros dos mecanismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAWFZKI86JP7"
   },
   "source": [
    "**MAR con respecto a `V1`**\n",
    "\n",
    "Dejamos la línea casi completa para el caso de `V2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juvs7nvF6JP8"
   },
   "outputs": [],
   "source": [
    "data.V2.isnull().groupby([data[COMPLETAR]]).sum().astype(int).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zteQhmA6JQK"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR PARA LOS ATRIBUTOS FALTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cWRpuSV6JQW"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR PARA LOS ATRIBUTOS FALTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzlSU6Gv6JQg"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR PARA LOS ATRIBUTOS FALTANTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGS1beU86JQ5"
   },
   "source": [
    "**MNAR**\n",
    "\n",
    "Las únicas variables correlacionadas son `V4` y `V5`, ambas con valores faltantes. Entonces, podemos usar una para ver si los valores faltantes de la otra son **MNAR**.\n",
    "\n",
    "Arranquemos estudiando los valores faltante en `V5`. Para ello, debemos binear `V4` y contar cuántos valores faltantes de `V5` caen en cada intervalo. Entonces, creamos los intervalos para `V4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajC_LYUX6JQ7"
   },
   "outputs": [],
   "source": [
    "data['V4-intervalos'] = pd.cut(data.V4, bins  = 15)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFwMUWHB6JRF"
   },
   "source": [
    "y contamos cuántas instancias por intervalo tienen valores faltantes en `V5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUx7joMw6JRG"
   },
   "outputs": [],
   "source": [
    "vf_V4V5 = data.V5.isnull().groupby([data['V4-intervalos']]).sum().astype(int).reset_index(name='count')\n",
    "vf_V4V5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zA0BM9n6JRR"
   },
   "source": [
    "Pero en realidad lo que nos interesa es la frecuencia de estos valores faltantes, porque no todos los intervalos tienen la misma cantidad de instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mdOX0Tl6JRT"
   },
   "outputs": [],
   "source": [
    "vf_V4V5['freq'] = vf_V4V5['count']/data.groupby([data['V4-intervalos']]).count()['V4'].values\n",
    "vf_V4V5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbCx4Fjq6JRd"
   },
   "source": [
    "Para que sea más fácil e informativo el gráfico, agregamos a este dataframe el punto medio de cada intervalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkWBGEZY6JRg"
   },
   "outputs": [],
   "source": [
    "vf_V4V5[\"bin_centres\"] = vf_V4V5[\"V4-intervalos\"].apply(lambda x: x.mid)\n",
    "vf_V4V5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvY5tpIL6JRp"
   },
   "source": [
    "y finalmente, graficamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xl1ISJ3r6JRr"
   },
   "outputs": [],
   "source": [
    "plt.scatter(vf_V4V5['bin_centres'], vf_V4V5['freq'])\n",
    "plt.xlabel('Punto medio bineado de V4')\n",
    "plt.ylabel('Frecuencia de valores faltantes en V5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajm379gm6JR0"
   },
   "source": [
    "¿Te parece que da alguna información este gráfico? No te apures a sacar conclusiones. Vuelve a repetir los pasos, pero invirtiendo el rol de `V5` y `V4` - solamente tienes que copiar cada celda e intercalar los nombres. Una vez que hayas llegado al gráfico, ahora sí mira si puedes sacar alguna conclusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rb88bap6JR2"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOwX6yYM6JR_"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owE4-2ro6JSG"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HleJyOu6JSN"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2ZR0NuT6JSW"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSh6Vs7X6JSc"
   },
   "source": [
    "¿Has llegado a alguna conclusión? Entre `V4` y `V5`, ¿cuál es MNAR y cuál MCAR?¿Y `V2`?\n",
    "\n",
    "## 1.2 Imputación de valores faltantes\n",
    "\n",
    "Vamos a *imputar* los valores faltantes sobre el dataset sintético y el dataset de Distrofia. \n",
    "\n",
    "### 1.2.1 Dataset Sintético\n",
    "\n",
    "Te recomendamos que:\n",
    "\n",
    "1. Al tratarse de un dataset pequeño, puedes copiar el dataset, dejar a un lado el original y rellenar valores faltantes en la copia. De esta forma, podrás evaluar tu estrategia (ver punto siguiente) y/o comparar distintas estrategias.\n",
    "1. Haz histogramas de las variables con y sin valores faltantes. Al imputar, ¿cambián las distribuciones?¿Esto está bien o mal?\n",
    "\n",
    "**Ejercicio 1 - Challenge 1:** Comienza con una estrategia sencilla, imputar con el valor medio de cada columna usando Pandas. ¿Para cuáles columnas tendrá sentido esto, teniendo en cuenta lo visto en el punto anterior? \n",
    "\n",
    "**Ejercicio 2:** Propone - e implementa - una estrategia que consideres mejor, incorporando la información del punto 1. En el siguiente encuentro, te daremos el dataset sin valores faltantes, por lo que podrás comparar tus resultados. ¡Ten en cuenta que es muy raro que esto se pueda hacer! Lo que nos lleva a...\n",
    "\n",
    "**Ejercicio 3:** ¿Se te ocurre algún mecanismo de evaluación de tu estrategia? Pista: considera lo que vimos sobre Machine Learning en el Bloque 1 de la carrera. \n",
    "\n",
    "Por las dudas, comenzamos volviendo a cargar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9qfHD7V6JSe"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('DS_Bitácora_19_Data_con_VF.csv')\n",
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNODKQe96JSm"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yAVpuxL6JSt"
   },
   "source": [
    "### 1.2.2 Distrofia\n",
    "\n",
    "La Distrofia Muscular de Duchenne (DMD) es una enfermedad que se transmite genéticamente de madres a hijos e hijas. Mientras que los varones mueren a temprana edad, las mujeres pueden llevar la enfermedad sin saberlo y sin síntomas. Se estima que 1 de cada 3300 mujeres son portadoras de la enfermedad.\n",
    "\n",
    "Si bien portadoras de DMD no suelen tener síntomas, tienden a exhibir elevados niveles de enzimas o proteínas del suero, como creatina quinasa (CK, por sus siglas en inglés), hemopexina (H), lactato deshidrogenasa (LD) y piruvato quinasa (PK, por sus siglas en inglés). Los niveles de estas enzimas también pueden variar con la edad y la estación. En un estudio llevado a cabo a finales de la década del 70 se midieron esas enzimas en portadoras y no-portadoras de la enfermedad, para estudiar la posibilidad de informar a mujeres la posibilidad de que sean portadoras. Los resultados los pueden encontrar en el archivo `DS_Bitácora_19_Distrofia.csv`.\n",
    "\n",
    "En este dataset hay valores faltantes en dos de sus columnas. Estos valores faltantes no aparecerán inmediatamente como `NaN` cuando abran el dataset con Pandas, sino que están guardados con un valor en particular. Abre y explora el dataset para descubrir ese valor. Luego, investiga como puedes pasar como argumento a la función de Pandas `read_csv` para que cuando cargue los datos ya los reconozca como valores faltantes. ¿Qué tipo de valores faltantes son?¿Están asociados a los valores de otras columnas? Luego, imputa los valores faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q73UO23m6JSv"
   },
   "outputs": [],
   "source": [
    "## COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAifc4I46JS3"
   },
   "source": [
    "## 2. Valores Atípicos\n",
    "\n",
    "Los datos que usaremos en esta sección, \"DS_Bitácora_19_Data_VA1.csv\" y \"DS_Bitácora_19_Data_VA2.csv\", fueron simulados con algún criterio desconocido. Uno de los archivos contiene \"valores atípicos\" y el otro no. El objetivo de esta sección es que decidan en cuál de ellos hay outliers y apliquen las técnicas vistas para detectarlos.\n",
    "\n",
    "Empieza por uno de los archivos, y, una vez que hayas terminado, repite para el otro.\n",
    "\n",
    "\n",
    "1. Exploración de datos.\n",
    "    1. Abrir los datos y explorarlos. Para ello, recomendamos hacer un `distplot` y un `boxplot`. ¿Hay valores atípicos? Si los hay, ¿en qué región del dominio?\n",
    "    2. Calcular algunos estadísticos sobre la muestra: valor medio, desviación estándar, cuartiles, máximo, mínimo, etc.\n",
    "2. Manejo de Outliers\n",
    "    1. Aplicar la técnica del rango interquartílico y la regla de las tres sigmas para seleccionar outliers. ¿Cuántos valores selecciona cada técnica?¿Seleccionan los mismos valores?\n",
    "    2. Supongamos que seleccionamos outliers y los descartamos. ¿Qué pasa si volvemos a aplicar estas técnicas?\n",
    "\n",
    "**1. Exploración de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acQYK0KL6JS5"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DICqcKP6JTB"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2ogvd3k6JTH"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u-4MvC76JTQ"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1Mi_xGc6JTW"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MboJWdu76JTc"
   },
   "source": [
    "**2. Manejo de outliers**\n",
    "\n",
    "No hay una sola forma de aplicar estas reglas.\n",
    "\n",
    "1. Rango Intercuartílico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fu4L6ld_6JTd"
   },
   "outputs": [],
   "source": [
    "q25,q75 = # COMPLETAR\n",
    "iqr = # COMPLETAR\n",
    "minimo = # COMPLETAR\n",
    "maximo = # COMPLETAR\n",
    "print(q25,q75,iqr, minimo, maximo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ONXmyWX6JTj"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMDDArAJ6JTp"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJU-d96B6JTw"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWOIUrwB6JT1"
   },
   "source": [
    "2. Regla de las tres sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvJdxpxX6JT2"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBmrgneQ6JT6"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBo8bRi16JUC"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQetScvn6JUI"
   },
   "source": [
    "## 3. Escalado de datos - Challenge 2\n",
    "\n",
    "Seguiremos usando los datos de la sección anterior:\n",
    "\n",
    "1. Agregar al dataframe una columna que se llame \"Datos_Reescalados\" y que contenga los datos reescalados por Z-Score. ¿Cuál es el valor medio de los datos reescalados?¿Y su desviación estándar? **Nota:** Para reescalar los datos, pueden hacerlo \"a mano\" o utilizar la clase `StandardScaler` de Scikit-Learn. No te olvides que las herramientas de preprocesamiento de datos de Scikit-learn tienen los mismos métodos (crear el objeto, fitearlo, transfromar los datos). Si necesitás ayuda, podés consultar la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "2. Realizá, en un mismo gráfico, un histograma de los datos reescaleados y un histograma de los datos crudos. ¿Qué similitudes y qué diferencias tiene con la distribución de datos crudos?\n",
    "3. Aplica la regla de las tres sigmas utilizando los datos reescalados.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5pfmTAz6JUK"
   },
   "outputs": [],
   "source": [
    "valor_medio = # COMPLETAR\n",
    "std = # COMPLETAR\n",
    "data['Datos_Reescalados'] = # COMPLETAR\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zbg6L8QG6JUP"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBmKdpHJ6JUV"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPHu-Hai6JUY"
   },
   "outputs": [],
   "source": [
    "mascara_outliers = # COMPLETAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5A6Aglk6JUd"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYHEMMgp6JUh"
   },
   "source": [
    "## 4. Encoders\n",
    "\n",
    "Elige un dataset de los que hayamos usado hasta el momento - puede ser el de Titanic, el de Vinos, el del proyecto o inclusive uno que hayas visto y que te interese - y haz los siguiente ejercicios:\n",
    "\n",
    "1. ¿De qué tipo es cada atributo?\n",
    "2. Lleva todos los atributos que te interesen a una forma numérica. ¿Cómo creció tu dataset?\n",
    "3. Si hay una tarea de regresión o clasificación bien definida sobre ese dataset que te interese explorar, hazlo. Para ello, entrena un modelo de árboles de decisión (no te olvides de seguir todos los pasos de un flujo de Machine Learning: elegir una métrica de evaluación, hacer una `train_test_split`, etc.). ¿Cómo cambia el desempeño del modelo con la incorporación de estos atributos? Recuerda que si utilizas un modelo de vecinos más cercanos, es importante escalar los datos antes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nAifc4I46JS3",
    "CQetScvn6JUI",
    "NYHEMMgp6JUh"
   ],
   "name": "DS_Bitácora_19_y_20_Preprocesamiento.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
